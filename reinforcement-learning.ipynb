{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gym\nimport numpy as np\n\n# Initialize the environment\nenv = gym.make('FrozenLake-v1', is_slippery=False)  # Setting is_slippery to False for deterministic behavior\n\n# Set parameters\nalpha = 0.8    # Learning rate\ngamma = 0.95   # Discount factor\nepsilon = 0.1  # Exploration rate\nnum_episodes = 2000\n\n# Initialize Q-table\nQ = np.zeros((env.observation_space.n, env.action_space.n))\n\n# Function for choosing an action using epsilon-greedy policy\ndef choose_action(state):\n    if np.random.uniform(0, 1) < epsilon:\n        return env.action_space.sample()\n    else:\n        return np.argmax(Q[state, :])\n\n# Q-Learning algorithm\nfor episode in range(num_episodes):\n    state = env.reset()\n    if isinstance(state, tuple):\n        state = state[0]  # Ensure state is an integer\n    \n    done = False\n    steps = 0\n\n    while not done:\n        action = choose_action(state)\n        step_result = env.step(action)\n        next_state, reward, done = step_result[0], step_result[1], step_result[2]\n        \n        if isinstance(next_state, tuple):\n            next_state = next_state[0]  # Ensure next_state is an integer\n\n        # Update Q-table\n        old_value = Q[state, action]\n        next_max = np.max(Q[next_state, :])\n        Q[state, action] = old_value + alpha * (reward + gamma * next_max - old_value)\n\n        state = next_state\n        steps += 1\n\n    if episode % 100 == 0:\n        print(f\"Episode {episode}: finished in {steps} steps\")\n\n# Display the learned Q-table\nprint(\"Learned Q-table:\")\nprint(Q)\n\n# Evaluate the learned policy\nstate = env.reset()\nif isinstance(state, tuple):\n    state = state[0]  # Ensure state is an integer\n\nenv.render()\ndone = False\ntotal_reward = 0\nwhile not done:\n    action = choose_action(state)\n    step_result = env.step(action)\n    next_state, reward, done = step_result[0], step_result[1], step_result[2]\n    \n    if isinstance(next_state, tuple):\n        next_state = next_state[0]  # Ensure next_state is an integer\n\n    state = next_state\n    total_reward += reward\n    env.render()\n\nprint(\"Total Reward:\", total_reward)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-22T23:30:15.398606Z","iopub.execute_input":"2024-07-22T23:30:15.399098Z","iopub.status.idle":"2024-07-22T23:30:23.145872Z","shell.execute_reply.started":"2024-07-22T23:30:15.399062Z","shell.execute_reply":"2024-07-22T23:30:23.144720Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n","output_type":"stream"},{"name":"stdout","text":"Episode 0: finished in 88 steps\nEpisode 100: finished in 203 steps\nEpisode 200: finished in 36 steps\nEpisode 300: finished in 12 steps\nEpisode 400: finished in 19 steps\nEpisode 500: finished in 4 steps\nEpisode 600: finished in 145 steps\nEpisode 700: finished in 60 steps\nEpisode 800: finished in 101 steps\nEpisode 900: finished in 170 steps\nEpisode 1000: finished in 153 steps\nEpisode 1100: finished in 46 steps\nEpisode 1200: finished in 80 steps\nEpisode 1300: finished in 41 steps\nEpisode 1400: finished in 223 steps\nEpisode 1500: finished in 110 steps\nEpisode 1600: finished in 160 steps\nEpisode 1700: finished in 29 steps\nEpisode 1800: finished in 89 steps\nEpisode 1900: finished in 62 steps\nLearned Q-table:\n[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\nTotal Reward: 0.0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gym/envs/toy_text/frozen_lake.py:271: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"FrozenLake-v1\", render_mode=\"rgb_array\")\u001b[0m\n  logger.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Reinforcement learning is a powerful approach for solving complex decision-making problems across various domains.**","metadata":{}}]}